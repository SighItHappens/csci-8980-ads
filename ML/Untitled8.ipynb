{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Untitled8.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_EN1Zh-3oDw",
        "outputId": "c30af69a-9e9a-4df2-ac81-359519906d66"
      },
      "source": [
        "!mkdir /content/drive/My\\ Drive/Distributed/\n",
        "\n",
        "%cd /content/drive/My\\ Drive/Distributed/"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/content/drive/My Drive/Distributed/’: File exists\n",
            "/content/drive/My Drive/Distributed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwHwL-5spy_w"
      },
      "source": [
        "from collections import defaultdict, deque, Counter\n",
        "import numpy as np\n",
        "from tqdm import tqdm_notebook as tqdm \n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import csv\n",
        "import pandas as pd\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import copy\n",
        "import torch\n",
        "from torch import nn\n",
        "import os\n",
        "import numpy as np\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import pickle as P\n",
        "from joblib import dump, load\n",
        "\n",
        "\n",
        "CACHE_SIZE= 50\n",
        "LR = 0.001\n",
        "BATCH_SIZE= 32\n",
        "EPOCH = 100\n",
        "PATH1 = \"CNN_2layer.pth\"\n",
        "PATH2 = \"MLP_2layer.pth\""
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJhZppBCp4qF",
        "cellView": "code"
      },
      "source": [
        "#@title Default title text\n",
        "def belady_opt(blocktrace, frame):\n",
        "    '''\n",
        "    INPUT\n",
        "    ============\n",
        "    blocktrace - list of blocks in sequence of request\n",
        "    cachesize - int value for capacity of the cache\n",
        "    \n",
        "    OUTPUT\n",
        "    ============\n",
        "    (1) hitrate (int)\n",
        "    (2) cache configuration and eviction block at time of miss (np.array)  \n",
        "    '''\n",
        "    \n",
        "    infinite_index = 100 * len(blocktrace) \n",
        "    # should be a large integer than block number\n",
        "    \n",
        "    block_index = defaultdict(deque) \n",
        "    # dictionary with block number as key and list\n",
        "    # of index value in blocktrace\n",
        "    \n",
        "    upcoming_index = defaultdict(int)\n",
        "    # dictionary with index number as key and value as block\n",
        "    \n",
        "    frequency = defaultdict(int)\n",
        "    # dictionary of block as key and number\n",
        "    # of times it's been requested so far\n",
        "    \n",
        "    recency = list()\n",
        "    # list of block in order of their request\n",
        "    \n",
        "    Cache = deque()\n",
        "    # Cache with block\n",
        "    \n",
        "    dataset = np.array([]).reshape(0,3*frame+1)\n",
        "    #columns represents the number of block in cache and \n",
        "    #3 is the number of features such as frequency, recency and block number\n",
        "    #+1 is for label 0-1\n",
        "    \n",
        "    hit, miss = 0, 0\n",
        "    \n",
        "    # populate the block_index\n",
        "    for i, block in enumerate(blocktrace):\n",
        "        block_index[block].append(i)\n",
        "        \n",
        "    # sequential block requests start\n",
        "    for i, block in enumerate(blocktrace):\n",
        "        \n",
        "        # increament the frequency number for the block\n",
        "        frequency[block] += 1\n",
        "        \n",
        "        # make sure block has the value in block_index dictionary \n",
        "        # as current seq_number\n",
        "        if len(block_index[block]) != 0 and block_index[block][0] == i:\n",
        "            \n",
        "            # if yes, remove the first element of block_index[block]\n",
        "            block_index[block].popleft()\n",
        "        \n",
        "        # if block exist in current cache\n",
        "        if block in Cache:\n",
        "            \n",
        "            # increment hit\n",
        "            hit += 1\n",
        "            \n",
        "            # update the recency\n",
        "            recency.remove(block)\n",
        "            recency.append(block)\n",
        "            \n",
        "            # update upcoming_index\n",
        "            if i in upcoming_index:\n",
        "                \n",
        "                # delete old index\n",
        "                del upcoming_index[i]\n",
        "        \n",
        "                if len(block_index[block]) is not 0:\n",
        "                    # add new upcoming index\n",
        "                    upcoming_index[block_index[block][0]] = block\n",
        "                    # remove index from block_index\n",
        "                    block_index[block].popleft()\n",
        "                else:\n",
        "                    # add a large integer as index\n",
        "                    upcoming_index[infinite_index] = block\n",
        "                    # increament large integer\n",
        "                    infinite_index-=1\n",
        "           \n",
        "        # block not in current cache\n",
        "        else:\n",
        "            \n",
        "            # increament miss\n",
        "            miss += 1\n",
        "            \n",
        "            # if cache has no free space\n",
        "            if len(Cache) == frame:\n",
        "                  \n",
        "                # find the farthest i.e. max_index in upcoming_index\n",
        "                max_index = max(upcoming_index)\n",
        "\n",
        "                if (i % 10 +1 == 10):\n",
        "                    blockNo = np.array([i for i in Cache])\n",
        "                    blockNo = blockNo / np.linalg.norm(blockNo)\n",
        "                    recency_ = np.array([recency.index(i) for i in Cache])\n",
        "                    recency_ = recency_ / np.linalg.norm(recency_)\n",
        "                    frequency_ = np.array([frequency[i] for i in Cache])\n",
        "                    frequency_ = frequency_ / np.linalg.norm(frequency_)\n",
        "                    stack = np.column_stack((blockNo, recency_, frequency_)).reshape(1,frame*3)\n",
        "                    stack = np.append(stack, Cache.index(upcoming_index[max_index]))\n",
        "                    dataset = np.vstack((dataset, stack))\n",
        "                # remove the block with max_index from cache\n",
        "                Cache[Cache.index(upcoming_index[max_index])] = block\n",
        "\n",
        "                # remove the block with max_index from recency dict\n",
        "                recency.remove(upcoming_index[max_index])\n",
        "\n",
        "                # remove max_index element from upcoming_index\n",
        "                del upcoming_index[max_index]\n",
        "                    \n",
        "            \n",
        "            else:\n",
        "                 \n",
        "                # add block into Cache\n",
        "                Cache.append(block)\n",
        "\n",
        "            # add block into recency\n",
        "            recency.append(block)\n",
        "                \n",
        "            # add upcoming request of current block in upcoming_index\n",
        "            if len(block_index[block]) != 0:\n",
        "                \n",
        "                # add upcoming index of block\n",
        "                upcoming_index[block_index[block][0]] = block\n",
        "               \n",
        "                # remove the index from block_index \n",
        "                block_index[block].popleft()\n",
        "            \n",
        "            else:\n",
        "                \n",
        "                # add a large integer as index\n",
        "                upcoming_index[infinite_index] = block\n",
        "                \n",
        "                # increament high number\n",
        "                infinite_index -= 1\n",
        " \n",
        "            \n",
        "    # calculate hitrate\n",
        "    hitrate = hit / (hit + miss)\n",
        "\n",
        "    return hitrate, dataset"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PAGRhrw4Ids"
      },
      "source": [
        "def LFU (blocktrace, cache_size):\n",
        "    cache = set()\n",
        "    cache_frequency = defaultdict(int)\n",
        "    frequency = defaultdict(int)\n",
        "\n",
        "    hit, miss = 0, 0\n",
        "\n",
        "    for block in tqdm(blocktrace, disable=True):\n",
        "        frequency[block] += 1\n",
        "\n",
        "        if block in cache:\n",
        "            hit += 1\n",
        "            cache_frequency[block] += 1\n",
        "\n",
        "        elif len(cache) < cache_size:\n",
        "            cache.add(block)\n",
        "            cache_frequency[block] += 1\n",
        "            miss += 1\n",
        "\n",
        "        else:\n",
        "            e, f = min(cache_frequency.items(), key=lambda a: a[1])\n",
        "            cache_frequency.pop(e)\n",
        "            cache.remove(e)\n",
        "            cache.add(block)\n",
        "            cache_frequency[block] = frequency[block]\n",
        "            miss += 1\n",
        "\n",
        "    hitrate = hit / (hit + miss)\n",
        "    return hitrate"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVGVpchFEohb"
      },
      "source": [
        "def LRU( blocktrace, cache_size):\n",
        "\n",
        "    cache = set()\n",
        "    recency = deque()\n",
        "    hit, miss = 0, 0\n",
        "\n",
        "    for block in tqdm(blocktrace, disable=True):\n",
        "\n",
        "        if block in cache:\n",
        "            recency.remove(block)\n",
        "            recency.append(block)\n",
        "            hit += 1\n",
        "\n",
        "        elif len(cache) < cache_size:\n",
        "            cache.add(block)\n",
        "            recency.append(block)\n",
        "            miss += 1\n",
        "\n",
        "        else:\n",
        "            cache.remove(recency[0])\n",
        "            recency.popleft()\n",
        "            cache.add(block)\n",
        "            recency.append(block)\n",
        "            miss += 1\n",
        "\n",
        "    hitrate = hit / (hit + miss)\n",
        "    return hitrate"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhMztE1gEjin"
      },
      "source": [
        "class LeCarLruLfu:\n",
        "\n",
        "    def __init__(self, learning_rate=0.45, discount_rate=0.005, cache_size=100):\n",
        "        self.learning_rate = learning_rate\n",
        "        # self.discount_rate=discount_rate ** (1/cache_size)\n",
        "        self.discount_rate = discount_rate\n",
        "        self.cache_size = cache_size\n",
        "        self.weight_lru = 0.5\n",
        "        self.weight_lfu = 0.5\n",
        "\n",
        "        self.history_size = self.cache_size * 1\n",
        "        self.cache = set()\n",
        "        self.page_timestamp = dict()\n",
        "        self.history_lru = deque()\n",
        "        self.history_lfu = deque()\n",
        "        self.global_frequency = defaultdict(int)\n",
        "\n",
        "        self.recency = deque()\n",
        "        self.cache_frequency = defaultdict(int)\n",
        "        self.frequency = defaultdict(int)\n",
        "\n",
        "        self.page_time = dict()\n",
        "\n",
        "        print(\"LeCarLruLfu: learning_rate:\" + str(self.learning_rate) + \" discount_rate:\" + str(\n",
        "            discount_rate) + \",\" + str(self.discount_rate) + \" cache_size:\" + str(self.cache_size))\n",
        "\n",
        "    def run_algorithm(self, blocktrace: list, timestamp: list):\n",
        "        size = len(blocktrace)\n",
        "\n",
        "        hit, miss = 0, 0\n",
        "        lru_miss = 0\n",
        "        lfu_miss = 0\n",
        "        total_lru_time = 0\n",
        "        total_lfu_time = 0\n",
        "        last_miss = 0\n",
        "        for i, block in enumerate(tqdm(blocktrace, disable=True)):\n",
        "            '''if i %1000==0:\n",
        "                #    print(str(i)+\"/\"+str(size)+\":\"+str(round(i/size,2))+\" time:\"+str(datetime.datetime.now()-start_time))\n",
        "                    print(\"reward:\"+str(reward_lru)+\",\"+str(reward_lfu))\n",
        "                    print(\"weight:\"+str(self.weight_lru)+\",\"+str(self.weight_lfu))\n",
        "             '''\n",
        "            self.frequency[block] += 1\n",
        "            if block in self.cache:\n",
        "                self.update_block(block)\n",
        "                hit += 1\n",
        "            else:\n",
        "                '''\n",
        "                if block in self.page_timestamp:\n",
        "                    time_spend_in_millis=self.millis(timestamp[i])-self.page_timestamp[block]\n",
        "                else:\n",
        "                    time_spend_in_millis=1'''\n",
        "                if block in self.history_lru or block in self.history_lfu:\n",
        "                    time_spend = i - self.page_time[block]\n",
        "\n",
        "                    if time_spend > miss - last_miss:\n",
        "                        if block in self.history_lru:\n",
        "                            self.history_lru.remove(block)\n",
        "                        if block in self.history_lfu:\n",
        "                            self.history_lfu.remove(block)\n",
        "                        last_miss = miss\n",
        "\n",
        "                # if self.reward_lru != 1 and self.reward_lfu !=1:\n",
        "                #    print(\"before reward:\"+str(self.reward_lru)+\",\"+str(self.reward_lfu))\n",
        "                reward_lru = 0\n",
        "                reward_lfu = 0\n",
        "\n",
        "                if block in self.history_lru:\n",
        "                    self.history_lru.remove(block)\n",
        "                    reward_lfu = self.discount_rate ** time_spend\n",
        "                    '''self.weight_lfu=self.weight_lfu* np.exp(self.learning_rate*reward_lfu)\n",
        "                    self.weight_lru=self.weight_lru/(self.weight_lfu+self.weight_lru)\n",
        "                    self.weight_lfu=1-self.weight_lru'''\n",
        "                    self.update_weight(reward_lru, reward_lfu)\n",
        "                    lru_miss += 1\n",
        "                    total_lru_time += time_spend\n",
        "\n",
        "                if block in self.history_lfu:\n",
        "                    self.history_lfu.remove(block)\n",
        "                    reward_lru = self.discount_rate ** time_spend\n",
        "                    '''self.weight_lru=self.weight_lru* np.exp(self.learning_rate*reward_lru)\n",
        "                    self.weight_lru=self.weight_lru/(self.weight_lfu+self.weight_lru)\n",
        "                    self.weight_lfu=1-self.weight_lru'''\n",
        "                    self.update_weight(reward_lru, reward_lfu)\n",
        "                    lfu_miss += 1\n",
        "                    total_lfu_time += time_spend\n",
        "\n",
        "                if len(self.cache) == self.cache_size:\n",
        "                    victim = self.evcitPage(block)\n",
        "                    self.cache.remove(victim)\n",
        "                    self.remove_block(victim)\n",
        "                self.cache.add(block)\n",
        "                self.add_block(block)\n",
        "                # self.page_timestamp[block]=self.millis(timestamp[i])\n",
        "                miss += 1\n",
        "            self.page_time[block] = i\n",
        "\n",
        "        print(\"weight:\" + str(self.weight_lru) + \",\" + str(self.weight_lfu))\n",
        "        print(\"lru_miss:\" + str(lru_miss) + \" lfu_miss:\" + str(lfu_miss) + \" total miss:\" + str(miss))\n",
        "        print(\"total_time_lru:\" + str(total_lru_time) + \" total_time_lfu:\" + str(total_lfu_time))\n",
        "        # print(\"avg lru time:\" + str(total_lru_time / lru_miss) + \" avg lfu time:\" + str(total_lfu_time / lfu_miss))\n",
        "\n",
        "        return hit / (hit + miss)\n",
        "\n",
        "    def update_weight(self, reward_lru, reward_lfu):\n",
        "        # if self.reward_lru != 1 or self.reward_lfu !=1:\n",
        "\n",
        "        # print(\"before reward:\"+str(self.reward_lru)+\",\"+str(self.reward_lfu))\n",
        "        self.weight_lfu = self.weight_lfu * np.exp(self.learning_rate * reward_lfu)\n",
        "        self.weight_lru = self.weight_lru * np.exp(self.learning_rate * reward_lru)\n",
        "        self.weight_lru = self.weight_lru / (self.weight_lfu + self.weight_lru)\n",
        "        self.weight_lfu = 1 - self.weight_lru\n",
        "        # if self.reward_lru != 1 or self.reward_lfu !=1:\n",
        "        #    print(\"after:\"+str(self.weight_lru)+\",\"+str(self.weight_lfu))\n",
        "        # print(\"after reward:\"+str(self.reward_lru)+\",\"+str(self.reward_lfu))\n",
        "\n",
        "    def millis(self, my_time):\n",
        "        return int(round(my_time * 1000))\n",
        "\n",
        "    def chooseRandom(self):\n",
        "        r = np.random.rand()\n",
        "        if r < self.weight_lru:\n",
        "            return 0\n",
        "        return 1\n",
        "\n",
        "    def evcitPage(self, block):\n",
        "        policy = self.chooseRandom()\n",
        "        if policy == 0:\n",
        "            victim = self.recency[0]\n",
        "            if len(self.history_lru) == self.history_size:\n",
        "                self.history_lru.popleft()\n",
        "            self.history_lru.append(victim)\n",
        "        else:\n",
        "            victim, f = min(self.cache_frequency.items(), key=lambda a: a[1])\n",
        "            if len(self.history_lfu) == self.history_size:\n",
        "                self.history_lfu.popleft()\n",
        "            self.history_lfu.append(victim)\n",
        "        return victim\n",
        "\n",
        "    def update_block(self, block):\n",
        "        self.cache_frequency[block] += 1\n",
        "        self.recency.remove(block)\n",
        "        self.recency.append(block)\n",
        "\n",
        "    def remove_block(self, block):\n",
        "        self.recency.remove(block)\n",
        "        self.cache_frequency.pop(block)\n",
        "\n",
        "    def add_block(self, block):\n",
        "        self.recency.append(block)\n",
        "        self.cache_frequency[block] = self.frequency[block]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAnnVOsvethA"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available else \"cpu\")\n",
        "\n",
        "# class CNN1(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(CNN1, self).__init__()\n",
        "#         self.dropout = nn.Dropout(0.5)\n",
        "#         self.out = nn.Sequential(\n",
        "#             nn.Linear(900,1000),\n",
        "#             # nn.ReLU(),\n",
        "#             # nn.Linear(1800,1000),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Linear(1000,CACHE_SIZE)\n",
        "#             )\n",
        "#     def forward(self, x):\n",
        "#         # x = self.conv1(x.unsqueeze(1))\n",
        "#         # x = self.conv2(x)\n",
        "#         # x = x.view(x.size(0), -1)           # flatten the output of conv2 to (batch_size, 32 * 7 * 7)\n",
        "#         output = self.out(x)\n",
        "#         return output\n",
        "\n",
        "class CNN2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN2, self).__init__()\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.layer1 = nn.Sequential(nn.Conv2d(\n",
        "            in_channels = 1,\n",
        "            out_channels = 4,\n",
        "            kernel_size = (7,1),\n",
        "            stride = 1,\n",
        "            padding =0),\n",
        "        nn.ReLU(),\n",
        "        self.dropout\n",
        "        )\n",
        "        self.layer2 = nn.Sequential(nn.Conv2d(\n",
        "            in_channels = 4,\n",
        "            out_channels = 8,\n",
        "            kernel_size = (7,3),\n",
        "            stride = 1,\n",
        "            padding =0\n",
        "            ),\n",
        "        nn.ReLU(),\n",
        "        self.dropout\n",
        "        )\n",
        "        self.fc = nn.Sequential( #704 for cachesize=100, 304 for 50\n",
        "            nn.Linear(304, 256)\n",
        "            )\n",
        "        self.out = nn.Sequential(\n",
        "            nn.Linear(256, CACHE_SIZE)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # print(x.shape)\n",
        "        x = self.layer1(x.view(-1, 1, CACHE_SIZE, 3))\n",
        "        x = self.layer2(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        # print(x.shape)\n",
        "        x = self.fc(x)\n",
        "        output = self.out(x)\n",
        "        return output\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29GT0r4-EMje"
      },
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Sequential(         # input shape (1, CACHE_SIZE, 2)\n",
        "            nn.Conv2d(\n",
        "                in_channels=1,              # input height\n",
        "                out_channels=5,            # n_filters\n",
        "                kernel_size=(5,3),              # filter size\n",
        "                stride=1,                   # filter movement/step\n",
        "                padding=0,                  # if want same width and length of this image after Conv2d, padding=(kernel_size-1)/2 if stride=1\n",
        "            ),                              # output shape (16, 28, 28)\n",
        "            nn.ReLU(),                      # activation\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(         # input shape (16, 14, 14)\n",
        "            nn.Conv2d(  \n",
        "                in_channels=5,              # input height\n",
        "                out_channels=10,            # n_filters\n",
        "                kernel_size=3,              # filter size\n",
        "                stride=1,                   # filter movement/step\n",
        "                padding=2),    # output shape (32, 14, 14)\n",
        "            nn.ReLU(),                      # activation\n",
        "        )\n",
        "        self.out = nn.Sequential(\n",
        "            nn.Linear(2940,1000),\n",
        "            nn.Linear(1000,CACHE_SIZE)\n",
        "            )\n",
        "        self.linear = nn.Sequential(\n",
        "            nn.Linear(3,10),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(10,100),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(100,1000),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1000,500),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(500,CACHE_SIZE)\n",
        "            )\n",
        "    def forward(self, x):\n",
        "        # x = self.conv1(x.unsqueeze(1))\n",
        "        # x = self.conv2(x)\n",
        "        # x = x.view(x.size(0), -1)           # flatten the output of conv2 to (batch_size, 32 * 7 * 7)\n",
        "        # output = self.out(x)\n",
        "        output = self.linear(x.view(-1, 1, CACHE_SIZE, 3))\n",
        "        return output"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KDBxliVEQTy"
      },
      "source": [
        "\n",
        "class LoadDataset(Dataset):\n",
        "  def __init__(self, x, y):\n",
        "    self.x = x\n",
        "    self.y = y\n",
        "  def __getitem__(self, item):\n",
        "    feature = self.x[item]\n",
        "    feature = feature.reshape(CACHE_SIZE, 3);\n",
        "    label = self.y[item]\n",
        "    return feature, label\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.x)\n",
        "\n",
        "\n",
        "\n",
        "def train(dataset, model, path):\n",
        "    train_data, val_data = train_test_split(dataset,test_size=0.1, random_state=None, shuffle=True)\n",
        "    trainset = LoadDataset(x=train_data[:,:-1], y=train_data[:, -1])\n",
        "    validset = LoadDataset(x=val_data[:,:-1], y=val_data[:, -1])\n",
        "\n",
        "    train_loader = DataLoader(trainset, batch_size = BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "    val_loader = DataLoader(validset, batch_size= BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "    lossplt =[]\n",
        "    epochplt=[]\n",
        "    valplt = []\n",
        "\n",
        "    pre_acc = 0.0\n",
        "    best_model = None\n",
        "    best_epoch =0\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "    # schedular = optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda= lambda epoch: 0.1)\n",
        "    \n",
        "    for epoch in range(0,EPOCH):\n",
        "        running_loss = 0.0\n",
        "        i=0\n",
        "        for train_X, train_Y in train_loader:\n",
        "            inputs, labels = train_X.to(device), train_Y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            outputs = model.forward(inputs.float())\n",
        "            loss = criterion(outputs, labels.long())\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "            # if(epoch%10 == 0):\n",
        "            #     schedular.step()\n",
        "        \n",
        "            running_loss += loss.item()\n",
        "\n",
        "        lossplt.append(running_loss)\n",
        "        epochplt.append(epoch)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            running_loss_val =0.0\n",
        "            for val_X, val_Y in val_loader:\n",
        "                inputs, labels = val_X.to(device), val_Y.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                \n",
        "                outputs = model.forward(inputs.float())\n",
        "                loss = criterion(outputs, labels.long())\n",
        "                running_loss_val += loss.item()\n",
        "            \n",
        "            valplt.append(running_loss_val)\n",
        "        acc,_,_ = predict(val_data, model)\n",
        "        print('[%d] Training loss: %3f' %(epoch+1, running_loss/len(train_data)),\n",
        "              ' [%d] Validation loss: %3f' %(epoch+1, running_loss_val/len(val_data)), ' With Validation Accuracy: %3f' %(acc))\n",
        "        \n",
        "        if(acc > pre_acc):\n",
        "            best_model = copy.deepcopy(model)\n",
        "            best_epoch = epoch\n",
        "            pre_acc = acc\n",
        "\n",
        "    print(\"Best model found at epoch \", best_epoch)\n",
        "    torch.save(best_model.state_dict(), path)\n",
        "    plt.plot(epochplt, lossplt)\n",
        "    plt.ylim([0, max(lossplt)+1])\n",
        "    plt.xlim([0, EPOCH+1])\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('loss')\n",
        "    plt.savefig('plot.png')\n",
        "\n",
        "\n",
        "def predict(test_data, model, device='cuda'):\n",
        "    # print(test_data.shape)\n",
        "    total =0\n",
        "    correct =0\n",
        "    pred_labels = []\n",
        "    orig_labels = []\n",
        "    testset = LoadDataset(x=test_data[:,:-1], y=test_data[:, -1])\n",
        "    test_loader= DataLoader(testset, batch_size=4, shuffle=False, num_workers=1)\n",
        "    print(device)\n",
        "    with torch.no_grad():\n",
        "        for test_data, test_label in test_loader:\n",
        "            if device == 'cpu':\n",
        "                inputs, labels = test_data, test_label\n",
        "            else:\n",
        "                inputs, labels = test_data.to(device), test_label.to(device)\n",
        "            outputs = model(inputs.float())\n",
        "            _, predicted = torch.max(F.softmax(outputs.data), 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted==labels).sum().item()\n",
        "            if device == 'cpu':\n",
        "                predicted = predicted.numpy().flatten()\n",
        "                labels = labels.numpy()\n",
        "            pred_labels.append(predicted)\n",
        "            orig_labels.append(labels)\n",
        "    return correct/total, pred_labels, orig_labels"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fBwNoaChKA1"
      },
      "source": [
        "def hitRate(blocktrace, frame, model, model_type='CNN'):\n",
        "    '''\n",
        "    INPUT\n",
        "    ==========\n",
        "    blocktrace = list of block request sequence\n",
        "    frame = size of the cache\n",
        "    \n",
        "    OUTPUT\n",
        "    ==========\n",
        "    hitrate \n",
        "    '''\n",
        "    \n",
        "    frequency = defaultdict(int)\n",
        "    # dictionary of block as key and number\n",
        "    # of times it's been requested so far\n",
        "    \n",
        "    recency = list()\n",
        "    # list of block in order of their request\n",
        "    \n",
        "    Cache = []\n",
        "    # Cache with block\n",
        "    \n",
        "    hit, miss = 0, 0\n",
        "    \n",
        "    \n",
        "    # sequential block requests start\n",
        "    for i, block in enumerate(blocktrace):\n",
        "        # increament the frequency number for the block\n",
        "        frequency[block] += 1\n",
        "        \n",
        "        # if block exist in current cache\n",
        "        if block in Cache:\n",
        "            \n",
        "            # increment hit\n",
        "            hit += 1\n",
        "            \n",
        "            # update the recency\n",
        "            recency.remove(block)\n",
        "            recency.append(block)\n",
        "            \n",
        "        # block not in current cache\n",
        "        else:\n",
        "            \n",
        "            # increament miss\n",
        "            miss += 1\n",
        "            \n",
        "            # if cache has no free space\n",
        "            if len(Cache) == frame:  \n",
        "                blockNo = np.array([i for i in Cache])\n",
        "                blockNo = blockNo / np.linalg.norm(blockNo)\n",
        "                recency_ = np.array([recency.index(i) for i in Cache])\n",
        "                recency_ = recency_ / np.linalg.norm(recency_)\n",
        "                frequency_ = np.array([frequency[i] for i in Cache])\n",
        "                frequency_ = frequency_ / np.linalg.norm(frequency_)\n",
        "                stack = np.column_stack((blockNo, recency_, frequency_)).reshape(1,frame*3)\n",
        "                if(model_type=='CNN'):\n",
        "                    index = model(torch.FloatTensor(stack))\n",
        "                    pred = np.argsort(F.softmax(index.data[0]).numpy())[-1]\n",
        "                else:\n",
        "                    # print(stack.shape)\n",
        "                    pred = int(model.predict(stack)[0])\n",
        "                    # print(pred)\n",
        "                # print(pred)\n",
        "                evict_block = Cache[pred]\n",
        "                # remove the block with max_index from cache\n",
        "                Cache[Cache.index(evict_block)] = block\n",
        "\n",
        "                # remove the block with max_index from recency dict\n",
        "                recency.remove(evict_block)\n",
        "            else:\n",
        "                # add block into Cache\n",
        "                Cache.append(block) \n",
        "            \n",
        "            # add block into recency\n",
        "            recency.append(block)  \n",
        "    # calculate hitrate\n",
        "    hitrate = hit / (hit + miss)\n",
        "\n",
        "    return hitrate"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiRQmQv7TEg4"
      },
      "source": [
        "# if __name__ == '__main__':\n",
        "# \tfilename = \"0.csv\"\n",
        "# \tblocktrace = []\n",
        "# \ttimestamp = []\n",
        "# \twith open(filename) as f:\n",
        "# \t\tcsv_reader = csv.reader(f, delimiter =\",\")\n",
        "# \t\tfor row in csv_reader:\n",
        "# \t\t\tblocktrace.append(int(row[1]))\n",
        "# \t\t\ttimestamp.append(int(row[0]))\n",
        "\n",
        "# \thr, dataset = belady_opt(blocktrace, CACHE_SIZE) #0.29212333333333335\n",
        "# \toutfile = open('blocktrace', 'wb')\n",
        "# \tP.dump(blocktrace,outfile)\n",
        "# \toutfile.close()\n",
        "# \toutfile = open('timestamp', 'wb')\n",
        "# \tP.dump(timestamp,outfile)\n",
        "# \toutfile.close()\n",
        "# \tprint(hr)\n",
        "# \tprint(\"blocktrace, timestamp are imported\")\n",
        "# \t# wfilename = \"data_interval10000.csv\"\n",
        "# \t# with open(wfilename, 'w', newline='') as f:\n",
        "# \t# \tfor data in dataset:\n",
        "# \t# \t\tcsv_writer = csv.writer(f, delimiter =\",\")\n",
        "# \t# \t\tcsv_writer.writerow(data)\n",
        "\n",
        "\n",
        "# # \t# dataset = np.array([]).reshape(0,3*CACHE_SIZE+1)\n",
        "# # \t# with open(\"data_interval10.csv\") as f:\n",
        "# # \t# \tcsv_reader = csv.reader(f, delimiter =\",\")\n",
        "# # \t# \ti =0\n",
        "# # \t# \tfor row in csv_reader:\n",
        "# # \t# \t\t# r = []\n",
        "# # \t# \t\t# for i in range(0,901):\n",
        "# # \t# \t\t# \tr.append(float(row[i]))\n",
        "# # \t# \t\trow[900] = int(float(row[900]))\n",
        "# # \t# \t\tdataset = np.vstack((dataset, row))\n",
        "# # \t# \t\tif(i%1000+1==1000):\n",
        "# # \t# \t\t\tprint(i, \" iterations done\")\n",
        "# # \t# \t\ti += 1\n",
        "# \toutfile = open('data_interval100', \"wb\")\n",
        "# \tP.dump(dataset,outfile)\n",
        "# \toutfile.close()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "r1GEuN9kiWYA",
        "outputId": "643a8cc9-8ed5-4c95-d66a-7ca22789f659"
      },
      "source": [
        "\n",
        "\n",
        "datablk = \"data/finaltrain2/0.csv\" #cheetah.cs.fiu.edu-110108-113008.2.blkparse\"\n",
        "\n",
        "df = pd.read_csv(datablk, sep =',')\n",
        "# print(df.head().T)\n",
        "# df = df.iloc[1:]\n",
        "# print(df.head().T)\n",
        "# df= df.str.split(\" \",  expand = True) \n",
        "# df.columns = ['timestamp','pid','pname','blockNo',\n",
        "#               'blockSize', 'readOrWrite', 'bdMajor', 'bdMinor', 'hash']\n",
        "\n",
        "df.columns = ['timestamp','blockNo', 'data', 'readOrWrite']\n",
        "\n",
        "blocktrace = df['blockNo'].tolist()\n",
        "\n",
        "timestamp = df['timestamp'].tolist()\n",
        "\n",
        "\n",
        "infile = open('csvblocktrace_train','wb')\n",
        "P.dump(blocktrace, infile)\n",
        "infile.close()\n",
        "\n",
        "infile = open('csvtimestamp_train','wb')\n",
        "P.dump(timestamp, infile)\n",
        "infile.close() \n",
        "\n",
        "hr, dataset = belady_opt(blocktrace, CACHE_SIZE)  #0.05295678027763743\n",
        "outfile = open('datacsv_train', \"wb\")\n",
        "P.dump(dataset,outfile)\n",
        "outfile.close()\n",
        "infile = open('csvblocktrace_train','rb')\n",
        "blocktrace = P.load(infile)\n",
        "infile.close()\n",
        "\n",
        "infile = open('csvtimestamp_train','rb')\n",
        "timestamp = P.load(infile)\n",
        "infile.close() \n",
        "\n",
        "infile = open('datacsv_train','rb')\n",
        "dataset = P.load(infile)\n",
        "infile.close() \n",
        "\n",
        "print(dataset.shape)\n",
        "\n",
        "# print(dataset[0])\n",
        "X_train = dataset\n",
        "bins = np.arange(0, CACHE_SIZE, 1)\n",
        "Y_train = X_train[:,-1]\n",
        "plt.xlim([min(Y_train), max(Y_train)])\n",
        "plt.hist(Y_train, bins=bins, alpha=0.5)\n",
        "plt.title('Histogram of label distribution')\n",
        "plt.xlabel('Cache id')\n",
        "plt.ylabel('count')\n",
        "plt.show()\n",
        "\n",
        "print(Y_train[:20])\n",
        "#Train and test the model\n",
        "print(X_train.shape)\n",
        "\n",
        "train_data = X_train \n",
        "model = CNN2()\n",
        "model.to(device)\n",
        "PATH = \"CNN_2_1layer_10.pth\"\n",
        "train(train_data, model, PATH)\n",
        "\n",
        "acc_train, _, _ = predict(train_data, model)\n",
        "print('Accuracy of CNN classifier on train set: {:.2f}'.format(acc_train))\n",
        "\n",
        "\n",
        "# model2 = CNN()\n",
        "# model2.to(device)\n",
        "# PATH2 = \"MLP_2layer.pth\"\n",
        "# train(train_data, model2, PATH2)\n",
        "\n",
        "\n",
        "# model2 = CNN()\n",
        "# model2.load_state_dict(torch.load(PATH2))\n",
        "# model2.eval()\n",
        "# acc_test, pred_labels, orig_labels = predict(test_data, model2)\n",
        "# acc_train, _, _ = predict(train_data, model2)\n",
        "# print('Accuracy of MLP classifier on test set: {:.2f}'.format(acc_test))\n",
        "# print('Accuracy of MLP classifier on train set: {:.2f}'.format(acc_train))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5483, 151)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[17. 18.  2. 44. 27. 19. 19. 49. 19. 32. 23. 18. 26.  9. 33. 40. 27.  7.\n",
            "  5. 44.]\n",
            "(5483, 151)\n",
            "cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:101: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1] Training loss: 0.122957  [1] Validation loss: 0.128425  With Validation Accuracy: 0.009107\n",
            "cuda\n",
            "[2] Training loss: 0.122767  [2] Validation loss: 0.128484  With Validation Accuracy: 0.007286\n",
            "cuda\n",
            "[3] Training loss: 0.122510  [3] Validation loss: 0.128047  With Validation Accuracy: 0.020036\n",
            "cuda\n",
            "[4] Training loss: 0.120357  [4] Validation loss: 0.125631  With Validation Accuracy: 0.056466\n",
            "cuda\n",
            "[5] Training loss: 0.116546  [5] Validation loss: 0.122384  With Validation Accuracy: 0.071038\n",
            "cuda\n",
            "[6] Training loss: 0.115124  [6] Validation loss: 0.123061  With Validation Accuracy: 0.072860\n",
            "cuda\n",
            "[7] Training loss: 0.113719  [7] Validation loss: 0.120655  With Validation Accuracy: 0.081967\n",
            "cuda\n",
            "[8] Training loss: 0.112919  [8] Validation loss: 0.122113  With Validation Accuracy: 0.076503\n",
            "cuda\n",
            "[9] Training loss: 0.111638  [9] Validation loss: 0.120246  With Validation Accuracy: 0.083789\n",
            "cuda\n",
            "[10] Training loss: 0.111700  [10] Validation loss: 0.119963  With Validation Accuracy: 0.069217\n",
            "cuda\n",
            "[11] Training loss: 0.110349  [11] Validation loss: 0.119067  With Validation Accuracy: 0.083789\n",
            "cuda\n",
            "[12] Training loss: 0.110386  [12] Validation loss: 0.118739  With Validation Accuracy: 0.076503\n",
            "cuda\n",
            "[13] Training loss: 0.109452  [13] Validation loss: 0.117548  With Validation Accuracy: 0.091075\n",
            "cuda\n",
            "[14] Training loss: 0.108810  [14] Validation loss: 0.119472  With Validation Accuracy: 0.092896\n",
            "cuda\n",
            "[15] Training loss: 0.107902  [15] Validation loss: 0.117936  With Validation Accuracy: 0.103825\n",
            "cuda\n",
            "[16] Training loss: 0.107260  [16] Validation loss: 0.120168  With Validation Accuracy: 0.116576\n",
            "cuda\n",
            "[17] Training loss: 0.106895  [17] Validation loss: 0.116871  With Validation Accuracy: 0.102004\n",
            "cuda\n",
            "[18] Training loss: 0.106617  [18] Validation loss: 0.115949  With Validation Accuracy: 0.087432\n",
            "cuda\n",
            "[19] Training loss: 0.106506  [19] Validation loss: 0.116203  With Validation Accuracy: 0.094718\n",
            "cuda\n",
            "[20] Training loss: 0.105809  [20] Validation loss: 0.119245  With Validation Accuracy: 0.103825\n",
            "cuda\n",
            "[21] Training loss: 0.105586  [21] Validation loss: 0.117820  With Validation Accuracy: 0.123862\n",
            "cuda\n",
            "[22] Training loss: 0.104712  [22] Validation loss: 0.116226  With Validation Accuracy: 0.122040\n",
            "cuda\n",
            "[23] Training loss: 0.104373  [23] Validation loss: 0.115027  With Validation Accuracy: 0.140255\n",
            "cuda\n",
            "[24] Training loss: 0.104327  [24] Validation loss: 0.114511  With Validation Accuracy: 0.129326\n",
            "cuda\n",
            "[25] Training loss: 0.103952  [25] Validation loss: 0.115741  With Validation Accuracy: 0.160291\n",
            "cuda\n",
            "[26] Training loss: 0.103710  [26] Validation loss: 0.115525  With Validation Accuracy: 0.140255\n",
            "cuda\n",
            "[27] Training loss: 0.102858  [27] Validation loss: 0.115748  With Validation Accuracy: 0.134791\n",
            "cuda\n",
            "[28] Training loss: 0.102915  [28] Validation loss: 0.116777  With Validation Accuracy: 0.149362\n",
            "cuda\n",
            "[29] Training loss: 0.102577  [29] Validation loss: 0.117851  With Validation Accuracy: 0.147541\n",
            "cuda\n",
            "[30] Training loss: 0.102519  [30] Validation loss: 0.114075  With Validation Accuracy: 0.162113\n",
            "cuda\n",
            "[31] Training loss: 0.100973  [31] Validation loss: 0.115936  With Validation Accuracy: 0.122040\n",
            "cuda\n",
            "[32] Training loss: 0.101281  [32] Validation loss: 0.113126  With Validation Accuracy: 0.154827\n",
            "cuda\n",
            "[33] Training loss: 0.100853  [33] Validation loss: 0.112155  With Validation Accuracy: 0.158470\n",
            "cuda\n",
            "[34] Training loss: 0.100003  [34] Validation loss: 0.114455  With Validation Accuracy: 0.142077\n",
            "cuda\n",
            "[35] Training loss: 0.100594  [35] Validation loss: 0.113024  With Validation Accuracy: 0.158470\n",
            "cuda\n",
            "[36] Training loss: 0.100293  [36] Validation loss: 0.112716  With Validation Accuracy: 0.142077\n",
            "cuda\n",
            "[37] Training loss: 0.099722  [37] Validation loss: 0.111874  With Validation Accuracy: 0.169399\n",
            "cuda\n",
            "[38] Training loss: 0.097732  [38] Validation loss: 0.112605  With Validation Accuracy: 0.173042\n",
            "cuda\n",
            "[39] Training loss: 0.098642  [39] Validation loss: 0.110885  With Validation Accuracy: 0.167577\n",
            "cuda\n",
            "[40] Training loss: 0.098205  [40] Validation loss: 0.112882  With Validation Accuracy: 0.160291\n",
            "cuda\n",
            "[41] Training loss: 0.098375  [41] Validation loss: 0.111361  With Validation Accuracy: 0.169399\n",
            "cuda\n",
            "[42] Training loss: 0.097650  [42] Validation loss: 0.113607  With Validation Accuracy: 0.162113\n",
            "cuda\n",
            "[43] Training loss: 0.098411  [43] Validation loss: 0.110456  With Validation Accuracy: 0.183971\n",
            "cuda\n",
            "[44] Training loss: 0.097517  [44] Validation loss: 0.110404  With Validation Accuracy: 0.180328\n",
            "cuda\n",
            "[45] Training loss: 0.098083  [45] Validation loss: 0.109162  With Validation Accuracy: 0.180328\n",
            "cuda\n",
            "[46] Training loss: 0.098036  [46] Validation loss: 0.109038  With Validation Accuracy: 0.187614\n",
            "cuda\n",
            "[47] Training loss: 0.097675  [47] Validation loss: 0.110142  With Validation Accuracy: 0.198543\n",
            "cuda\n",
            "[48] Training loss: 0.097261  [48] Validation loss: 0.110204  With Validation Accuracy: 0.169399\n",
            "cuda\n",
            "[49] Training loss: 0.097207  [49] Validation loss: 0.113622  With Validation Accuracy: 0.193078\n",
            "cuda\n",
            "[50] Training loss: 0.097251  [50] Validation loss: 0.110642  With Validation Accuracy: 0.196721\n",
            "cuda\n",
            "[51] Training loss: 0.097321  [51] Validation loss: 0.109792  With Validation Accuracy: 0.189435\n",
            "cuda\n",
            "[52] Training loss: 0.096359  [52] Validation loss: 0.109996  With Validation Accuracy: 0.191257\n",
            "cuda\n",
            "[53] Training loss: 0.097393  [53] Validation loss: 0.108969  With Validation Accuracy: 0.209472\n",
            "cuda\n",
            "[54] Training loss: 0.096983  [54] Validation loss: 0.111521  With Validation Accuracy: 0.213115\n",
            "cuda\n",
            "[55] Training loss: 0.097234  [55] Validation loss: 0.107616  With Validation Accuracy: 0.189435\n",
            "cuda\n",
            "[56] Training loss: 0.097031  [56] Validation loss: 0.107894  With Validation Accuracy: 0.211293\n",
            "cuda\n",
            "[57] Training loss: 0.096741  [57] Validation loss: 0.111656  With Validation Accuracy: 0.216758\n",
            "cuda\n",
            "[58] Training loss: 0.097133  [58] Validation loss: 0.112899  With Validation Accuracy: 0.209472\n",
            "cuda\n",
            "[59] Training loss: 0.096455  [59] Validation loss: 0.111070  With Validation Accuracy: 0.185792\n",
            "cuda\n",
            "[60] Training loss: 0.096041  [60] Validation loss: 0.112049  With Validation Accuracy: 0.213115\n",
            "cuda\n",
            "[61] Training loss: 0.096457  [61] Validation loss: 0.108124  With Validation Accuracy: 0.205829\n",
            "cuda\n",
            "[62] Training loss: 0.096262  [62] Validation loss: 0.108173  With Validation Accuracy: 0.171220\n",
            "cuda\n",
            "[63] Training loss: 0.095376  [63] Validation loss: 0.110807  With Validation Accuracy: 0.207650\n",
            "cuda\n",
            "[64] Training loss: 0.094855  [64] Validation loss: 0.110969  With Validation Accuracy: 0.234973\n",
            "cuda\n",
            "[65] Training loss: 0.095835  [65] Validation loss: 0.107347  With Validation Accuracy: 0.196721\n",
            "cuda\n",
            "[66] Training loss: 0.095163  [66] Validation loss: 0.110346  With Validation Accuracy: 0.182149\n",
            "cuda\n",
            "[67] Training loss: 0.094465  [67] Validation loss: 0.109532  With Validation Accuracy: 0.176685\n",
            "cuda\n",
            "[68] Training loss: 0.095677  [68] Validation loss: 0.110887  With Validation Accuracy: 0.220401\n",
            "cuda\n",
            "[69] Training loss: 0.095834  [69] Validation loss: 0.113585  With Validation Accuracy: 0.224044\n",
            "cuda\n",
            "[70] Training loss: 0.095172  [70] Validation loss: 0.107061  With Validation Accuracy: 0.193078\n",
            "cuda\n",
            "[71] Training loss: 0.095736  [71] Validation loss: 0.109448  With Validation Accuracy: 0.209472\n",
            "cuda\n",
            "[72] Training loss: 0.094387  [72] Validation loss: 0.107911  With Validation Accuracy: 0.216758\n",
            "cuda\n",
            "[73] Training loss: 0.094855  [73] Validation loss: 0.109757  With Validation Accuracy: 0.233151\n",
            "cuda\n",
            "[74] Training loss: 0.095164  [74] Validation loss: 0.108430  With Validation Accuracy: 0.187614\n",
            "cuda\n",
            "[75] Training loss: 0.095523  [75] Validation loss: 0.113033  With Validation Accuracy: 0.209472\n",
            "cuda\n",
            "[76] Training loss: 0.095770  [76] Validation loss: 0.111070  With Validation Accuracy: 0.211293\n",
            "cuda\n",
            "[77] Training loss: 0.095267  [77] Validation loss: 0.107366  With Validation Accuracy: 0.193078\n",
            "cuda\n",
            "[78] Training loss: 0.094675  [78] Validation loss: 0.112037  With Validation Accuracy: 0.207650\n",
            "cuda\n",
            "[79] Training loss: 0.094559  [79] Validation loss: 0.109144  With Validation Accuracy: 0.218579\n",
            "cuda\n",
            "[80] Training loss: 0.094221  [80] Validation loss: 0.107929  With Validation Accuracy: 0.213115\n",
            "cuda\n",
            "[81] Training loss: 0.094684  [81] Validation loss: 0.111566  With Validation Accuracy: 0.238616\n",
            "cuda\n",
            "[82] Training loss: 0.094847  [82] Validation loss: 0.109417  With Validation Accuracy: 0.218579\n",
            "cuda\n",
            "[83] Training loss: 0.095146  [83] Validation loss: 0.111043  With Validation Accuracy: 0.225865\n",
            "cuda\n",
            "[84] Training loss: 0.094636  [84] Validation loss: 0.109673  With Validation Accuracy: 0.214936\n",
            "cuda\n",
            "[85] Training loss: 0.093653  [85] Validation loss: 0.107920  With Validation Accuracy: 0.209472\n",
            "cuda\n",
            "[86] Training loss: 0.094179  [86] Validation loss: 0.108173  With Validation Accuracy: 0.205829\n",
            "cuda\n",
            "[87] Training loss: 0.094361  [87] Validation loss: 0.110519  With Validation Accuracy: 0.220401\n",
            "cuda\n",
            "[88] Training loss: 0.095126  [88] Validation loss: 0.112500  With Validation Accuracy: 0.216758\n",
            "cuda\n",
            "[89] Training loss: 0.093806  [89] Validation loss: 0.111197  With Validation Accuracy: 0.205829\n",
            "cuda\n",
            "[90] Training loss: 0.094294  [90] Validation loss: 0.112587  With Validation Accuracy: 0.224044\n",
            "cuda\n",
            "[91] Training loss: 0.093917  [91] Validation loss: 0.108712  With Validation Accuracy: 0.207650\n",
            "cuda\n",
            "[92] Training loss: 0.094373  [92] Validation loss: 0.109189  With Validation Accuracy: 0.211293\n",
            "cuda\n",
            "[93] Training loss: 0.094998  [93] Validation loss: 0.108561  With Validation Accuracy: 0.200364\n",
            "cuda\n",
            "[94] Training loss: 0.094954  [94] Validation loss: 0.110026  With Validation Accuracy: 0.207650\n",
            "cuda\n",
            "[95] Training loss: 0.094063  [95] Validation loss: 0.109338  With Validation Accuracy: 0.180328\n",
            "cuda\n",
            "[96] Training loss: 0.092697  [96] Validation loss: 0.107481  With Validation Accuracy: 0.183971\n",
            "cuda\n",
            "[97] Training loss: 0.093966  [97] Validation loss: 0.108119  With Validation Accuracy: 0.209472\n",
            "cuda\n",
            "[98] Training loss: 0.094420  [98] Validation loss: 0.106587  With Validation Accuracy: 0.220401\n",
            "cuda\n",
            "[99] Training loss: 0.094213  [99] Validation loss: 0.110106  With Validation Accuracy: 0.196721\n",
            "cuda\n",
            "[100] Training loss: 0.093755  [100] Validation loss: 0.111652  With Validation Accuracy: 0.214936\n",
            "Best model found at epoch  80\n",
            "cuda\n",
            "Accuracy of CNN classifier on train set: 0.25\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BT2lFqLthPWW",
        "outputId": "fa12c9a2-20da-4815-cbcd-5691047deedc"
      },
      "source": [
        "NN = MLPClassifier(hidden_layer_sizes=(500, ), activation='tanh', batch_size=64, random_state=1, max_iter=500)\n",
        "NN.fit(X_train[:, :-1], Y_train)\n",
        "\n",
        "dump(NN, 'MLP_10.joblib') "
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['MLP_10.joblib']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0Ierdt5hx8I",
        "outputId": "430a7885-d26f-43ea-9889-72b582082da5"
      },
      "source": [
        "logreg = LogisticRegression(max_iter = 10000)\n",
        "logreg.fit(X_train[:,:-1], Y_train)\n",
        "\n",
        "dump(logreg, 'LR_10.joblib') "
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['LR_10.joblib']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCAHACF6ZMkz",
        "outputId": "4b4d8ef7-7775-4416-fab8-5a20750c2c83"
      },
      "source": [
        "datablk = \"data/finaltest2/0.csv\" #cheetah.cs.fiu.edu-110108-113008.2.blkparse\"\n",
        "df = pd.read_csv(datablk, sep =',')\n",
        "df.columns = ['timestamp','blockNo', 'data', 'readOrWrite']\n",
        "\n",
        "blocktrace_test = df['blockNo'].tolist()\n",
        "\n",
        "timestamp_test = df['timestamp'].tolist()\n",
        "\n",
        "\n",
        "infile = open('csvblocktrace_test','wb')\n",
        "P.dump(blocktrace_test, infile)\n",
        "infile.close()\n",
        "\n",
        "infile = open('csvtimestamp_test','wb')\n",
        "P.dump(timestamp_test, infile)\n",
        "infile.close() \n",
        "\n",
        "hr_test, dataset_test = belady_opt(blocktrace_test, CACHE_SIZE)  #0.05295678027763743\n",
        "outfile = open('datacsv_test', \"wb\")\n",
        "P.dump(dataset_test,outfile)\n",
        "outfile.close()\n",
        "infile = open('csvblocktrace_test','rb')\n",
        "blocktrace_test = P.load(infile)\n",
        "infile.close()\n",
        "\n",
        "infile = open('csvtimestamp_test','rb')\n",
        "timestamp_test = P.load(infile)\n",
        "infile.close() \n",
        "\n",
        "infile = open('datacsv_test','rb')\n",
        "dataset_test = P.load(infile)\n",
        "infile.close() \n",
        "\n",
        "\n",
        "X_test = dataset_test\n",
        "Y_test = X_test[:,-1]\n",
        "\n",
        "#hitrate Computation\n",
        "model = CNN2()\n",
        "model.eval()\n",
        "# model.to(device)\n",
        "model.load_state_dict(torch.load(PATH))\n",
        "\n",
        "acc_test, pred_labels, orig_labels = predict(X_test, model, device='cpu')\n",
        "print('Accuracy of CNN classifier on test set: {:.2f}'.format(acc_test))\n",
        "\n",
        "print(hr_test)\n",
        "model.load_state_dict(torch.load(PATH, map_location='cpu'))\n",
        "hr_CNN = hitRate(blocktrace, CACHE_SIZE, model) #0.33\n",
        "print('Hitrate CNN classifier workload: {:.2f}'.format(hr_CNN))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cpu\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:101: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy of CNN classifier on test set: 0.33\n",
            "0.44914491449144917\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:58: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Hitrate CNN classifier workload: 0.21\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_QEnSeXh6ET",
        "outputId": "2ea07fe4-21e1-4fba-9b83-d4a69349dbf0"
      },
      "source": [
        "MLP = load('MLP_10.joblib')\n",
        "print('MLP Score: ',MLP.score(X_test[:,:-1], Y_test))\n",
        "\n",
        "hitRate(blocktrace_test, CACHE_SIZE, MLP, model_type='MLP')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MLP Score:  0.22735674676524953\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.2528252825282528"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coyds3gtjjKZ",
        "outputId": "dc9d9763-d33c-462d-d87b-2478e4ac3c8e"
      },
      "source": [
        "LReg = load('LR_10.joblib')\n",
        "print('LR Score: ',LReg.score(X_test[:,:-1], Y_test))\n",
        "\n",
        "hitRate(blocktrace_test, CACHE_SIZE, LReg, model_type='LR')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LR Score:  0.2033271719038817\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.25032503250325033"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8IVr5ttseTPr",
        "outputId": "3f5c1e8c-cd6e-4a96-f07d-952b02fbcc65"
      },
      "source": [
        "print(LRU(blocktrace_test, CACHE_SIZE)) #0.2846284628462846\n",
        "print(LFU(blocktrace_test, CACHE_SIZE)) #0.4215421542154215\n",
        "LeCar = LeCarLruLfu(cache_size=CACHE_SIZE) \n",
        "print(LeCar.run_algorithm(blocktrace_test, timestamp_test)) #0.3256325632563256"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.15351535153515353\n",
            "0.3248324832483248\n",
            "LeCarLruLfu: learning_rate:0.45 discount_rate:0.005,0.005 cache_size:50\n",
            "weight:0.5000000141328125,0.4999999858671875\n",
            "lru_miss:5 lfu_miss:6 total miss:8014\n",
            "total_time_lru:374 total_time_lfu:164\n",
            "0.19851985198519853\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  import sys\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:36: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okluRhg1LEmy"
      },
      "source": [
        "# print(LRU(blocktrace, CACHE_SIZE)) #0.04229397072599403\n",
        "# print(LFU(blocktrace, CACHE_SIZE)) #0.04168193991750793\n",
        "# LeCar = LeCarLruLfu(cache_size=CACHE_SIZE) \n",
        "# print(LeCar.run_algorithm(blocktrace, timestamp)) #0.04256559792437897"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlgeHhYMVJdy"
      },
      "source": [
        "eviction = int(0.2*100)\n",
        "def hitRate2(blocktrace, frame, model):\n",
        "    LFUDict = defaultdict(int)\n",
        "    LRUQ = []\n",
        "#     CacheTS = defaultdict(int)\n",
        "#     CachePID = defaultdict(int)\n",
        "\n",
        "    hit, miss = 0, 0\n",
        "\n",
        "    C = []\n",
        "    evictCacheIndex = np.array([])\n",
        "    #count=0\n",
        "    #seq_number = 0\n",
        "    for seq_number, block in enumerate(tqdm(blocktrace, desc=\"OPT\")):\n",
        "        #print(len(evictCacheIndex))\n",
        "        LFUDict[block] +=1\n",
        "        #CacheTS[blocktrace[seq_number]] = timestamp[seq_number]\n",
        "        #CachePID[blocktrace[seq_number]] = pid[seq_number]\n",
        "        if block in C:\n",
        "            hit+=1\n",
        "#             if C.index(block) in evictCacheIndex:\n",
        "#                 np.delete(evictCacheIndex, C.index(block))\n",
        "                \n",
        "            LRUQ.remove(block)\n",
        "            LRUQ.append(block)\n",
        "        else:\n",
        "            evictPos = -1\n",
        "            miss+=1\n",
        "            if len(C) == frame:\n",
        "                if len(evictCacheIndex) == 0: # call eviction candidates\n",
        "                    #X_test = getX(LRUQ, LFUDict, C)\n",
        "                    #X_test = getX(LRUQ, LFUDict, C, CacheTS, CachePID)\n",
        "                    blockNo = C / np.linalg.norm(C)\n",
        "                    recency_ = np.array([LRUQ.index(i) for i in C])\n",
        "                    recency_ = recency_ / np.linalg.norm(recency_)\n",
        "                    frequency_ = np.array([LFUDict[i] for i in C])\n",
        "                    frequency_ = frequency_ / np.linalg.norm(frequency_)\n",
        "                    stack = np.column_stack((blockNo, recency_, frequency_)).reshape(1,frame*3)\n",
        "                    #X_current = model.predict(stack)[0]\n",
        "                    index = model(torch.FloatTensor(stack))\n",
        "                    Y_pred_prob = F.softmax(index.data[0]).numpy()\n",
        "                    # Y_pred_prob = model.predict_proba(stack) #sklearn\n",
        "                    evictCacheIndex = Y_pred_prob.argsort()[::-1][:eviction]\n",
        "                    # index of cache blocks that should be removed\n",
        "                    #return Y_pred_prob, evictCacheIndex\n",
        "                # evict from cache\n",
        "                evictPos = evictCacheIndex[0]\n",
        "                evictBlock = C[evictPos]\n",
        "                LRUQ.remove(evictBlock)\n",
        "                #del CacheTS [evictBlock]\n",
        "                #del CachePID [evictBlock]\n",
        "            if evictPos is -1:\n",
        "                C.append(block)\n",
        "            else:\n",
        "                C[evictPos] = block\n",
        "                evictCacheIndex = np.delete(evictCacheIndex, 0)\n",
        "            LRUQ.append(block)\n",
        "            #CacheTS [blocktrace[seq_number]] = timestamp[seq_number]\n",
        "            #CachePID [blocktrace[seq_number]] = pid[seq_number]\n",
        "        #seq_number += 1\n",
        "\n",
        "    hitrate = hit / (hit + miss)\n",
        "    print(hitrate)\n",
        "    return hitrate\n",
        "\n",
        "hitRate2(blocktrace_test, 100, model)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}